{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO0pA5QIgjMPSkwlm7uU7lj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zKYkEi3tgRRz","executionInfo":{"status":"ok","timestamp":1715545993911,"user_tz":420,"elapsed":14995,"user":{"displayName":"JUN KANG","userId":"05514736638830629409"}},"outputId":"43718a1f-07c5-46bb-8302-e66c30b5408c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/persona-chat\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/MyDrive/persona-chat/spc_dataset/"]},{"cell_type":"code","source":["# split\n","import json\n","from sklearn.model_selection import train_test_split\n","\n","with open('spc_dataset.json', 'r') as file:\n","    data = json.load(file)\n","\n","train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n","\n","with open('train_spc_dataset.json', 'w') as file:\n","    json.dump(train_data, file)\n","with open('val_spc_dataset.json', 'w') as file:\n","    json.dump(val_data, file)"],"metadata":{"id":"iWJOYytqiwtA","executionInfo":{"status":"ok","timestamp":1715546670767,"user_tz":420,"elapsed":1963,"user":{"displayName":"JUN KANG","userId":"05514736638830629409"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from transformers import GPTNeoXTokenizerFast\n","\n","tokenizer = GPTNeoXTokenizerFast.from_pretrained('EleutherAI/gpt-neox-20b')\n","\n","def load_data(filename):\n","    with open(filename, 'r') as file:\n","        data = json.load(file)\n","    return data\n","\n","def tokenize_spc(data, tokenizer):\n","    tokenized_data = []\n","    for item in data:\n","        history = \"<History>\"\n","        user1_persona = f\"<Persona 1> {item['user1_persona']}\"\n","        user2_persona = f\"<Persona 2> {item['user2_persona']}\"\n","        for convo in item['conversations']:\n","            user_text = f\"<User> {convo['user']}\"\n","            bot_text = f\"<Bot> {convo['bot']}\"\n","            conversation = f\"{user1_persona} {user2_persona} {history} {user_text} {bot_text}\"\n","            conversation_sequence = tokenizer.encode(conversation, truncation=True, max_length=2048)\n","            tokenized_data.append(conversation_sequence)\n","            history += f\" {convo['user']} {convo['bot']} \"\n","    return tokenized_data\n","\n","data1 = load_data('spc_dataset.json')\n","tokenized_data1 = tokenize_spc(data1, tokenizer)\n","\n","for encoded_input in tokenized_data1[:5]:\n","    print(encoded_input)\n","    decoded_text1 = tokenizer.decode(encoded_input)\n","    print(decoded_text1)"],"metadata":{"id":"gyyV9RoJjKEq"},"execution_count":null,"outputs":[]}]}